from google.cloud import bigquery
client = bigquery.Client()
bucket_name = 'temporary_clevertap_export'
project = "zeniusnet-d7638"
dataset_id = "zenius_clevertap"
table_id = "shakespeare" #[TODO]
tables = ['custom_control_group', 'historical_clevertap', 'journey_custom_control_group', 'journey_system_control_group', 'notification_clicked', 'notification_sent', 'notification_viewed', 'push_impressions', 'sample_notification_sent', 'sample_notification_viewed', 'system_control_group']
job_config = bigquery.job.ExtractJobConfig()
job_config.destination_format = bigquery.DestinationFormat.AVRO
job_config.compression = bigquery.Compression.SNAPPY
dataset_ref = bigquery.DatasetReference(project, dataset_id)

for i in tables:
    table_ref = dataset_ref.table(i)
    destination_uri = "gs://{}/{}/*".format(bucket_name, i)
    extract_job = client.extract_table(
        table_ref,
        destination_uri,
        # Location must match that of the source table.
        job_config=job_config,
        location="asia-southeast1",
    )  # API request
    extract_job.result()  # Waits for job to complete.

    print(
        "Exported {}:{}.{} to {}".format(project, dataset_id, i, destination_uri))